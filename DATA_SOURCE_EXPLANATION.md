# ğŸ“Š Where Does the Data Come From?

## ğŸ¯ Quick Answer

The data you see in the Control Panel comes from **two sources**:

1. **Sample/Simulated Data** - Generated by `backend/data_generator.py`
2. **Real Data** - Fetched from Reddit, Twitter, and News RSS feeds

---

## ğŸ“ˆ Control Panel Statistics

### **What You See:**
```
Damaged Buildings: 1,247
Flooded Zones: 18
Displaced People: 12,500
Emergency Shelters: 15
```

### **Where It Comes From:**

**File:** `backend/main.py` (lines 230-242)

```python
@app.get("/api/statistics")
async def get_statistics():
    """Get disaster statistics dashboard"""
    stats = {
        "total_affected_area_km2": 245.7,
        "damaged_buildings": 1247,        # â† This number
        "flooded_zones": 18,              # â† This number
        "displaced_population": 12500,    # â† This number
        "rescue_operations_active": 23,
        "emergency_shelters": 15,         # â† This number
        "last_updated": datetime.now().isoformat()
    }
    return stats
```

**Status:** These are **hardcoded sample values** for demo purposes.

---

## ğŸ—ºï¸ Map Data (Zones, Floods, Infrastructure)

### **What You See:**
- Red markers: Disaster zones
- Blue markers: Flood areas
- Orange markers: Infrastructure damage
- Purple markers: Displacement zones

### **Where It Comes From:**

**File:** `backend/data_generator.py`

```python
class DataGenerator:
    def generate_disaster_zones(self):
        """Generate realistic disaster zone data"""
        zones = []
        for i in range(15):
            zone = {
                "id": f"zone_{i+1}",
                "name": f"Zone {chr(65+i)}",
                "severity": random.choice(["critical", "high", "medium", "low"]),
                "damage_score": random.uniform(0.3, 0.95),
                "affected_area_km2": random.uniform(5, 50),
                "coordinates": {
                    "lat": 19.0760 + random.uniform(-0.1, 0.1),
                    "lon": 72.8777 + random.uniform(-0.1, 0.1)
                }
            }
            zones.append(zone)
        return zones
```

**Status:** These are **algorithmically generated** based on realistic patterns.

---

## ğŸ“± Social Media Feed

### **What You See:**
Posts like:
- "Urgent help needed in Bandra..."
- "Flooding reported near..."
- Reddit posts about disasters

### **Where It Comes From:**

**Two Sources:**

#### **1. Real Data (Background Fetched)**
**File:** `backend/social_media_scraper.py`

```python
class SocialMediaScraper:
    def get_all_social_media(self):
        """Fetch REAL data from Reddit, Twitter, News"""
        all_posts = []
        
        # Reddit
        reddit_posts = self.scrape_reddit()
        
        # Twitter (via Nitter)
        twitter_posts = self.scrape_nitter()
        
        # News RSS
        news_posts = self.scrape_news_rss()
        
        return {
            'posts': all_posts,
            'sources': ['Reddit', 'Twitter', 'News RSS']
        }
```

**How it works:**
- Runs in background thread (every 60 seconds)
- Fetches from Reddit API
- Scrapes Twitter via Nitter
- Parses news RSS feeds
- Caches results for fast frontend access

#### **2. Sample Data (Instant)**
**File:** `backend/data_generator.py`

```python
def generate_social_feed(self):
    """Generate sample social media posts"""
    posts = []
    templates = [
        "Urgent help needed in {location}. {people} people trapped.",
        "Flooding reported near {location}. Water level rising.",
        "Building collapsed in {location}. Rescue teams needed."
    ]
    
    for template in templates:
        post = {
            "text": template.format(
                location=random.choice(locations),
                people=random.randint(5, 50)
            ),
            "urgency": random.choice(["critical", "high", "medium"]),
            "timestamp": datetime.now().isoformat()
        }
        posts.append(post)
    
    return posts
```

**Combined in API:**
```python
@app.get("/api/social-feed")
async def get_social_feed():
    # Get cached REAL data
    real_posts = social_media_cache.get("posts", [])
    
    # Get SAMPLE data
    sample_posts = data_generator.generate_social_feed()
    
    # Combine both
    all_posts = real_posts + sample_posts
    
    return {
        "posts": all_posts,
        "real_count": len(real_posts),
        "sample_count": len(sample_posts)
    }
```

---

## ğŸ”„ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FRONTEND                       â”‚
â”‚  (React App - http://localhost:3000)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ HTTP Requests
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              BACKEND API                        â”‚
â”‚  (FastAPI - http://localhost:8000)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  GET /api/statistics                           â”‚
â”‚  â”œâ”€â†’ Returns hardcoded stats                   â”‚
â”‚                                                 â”‚
â”‚  GET /api/disaster-zones                       â”‚
â”‚  â”œâ”€â†’ data_generator.generate_disaster_zones()  â”‚
â”‚                                                 â”‚
â”‚  GET /api/social-feed                          â”‚
â”‚  â”œâ”€â†’ Cached real data (background thread)      â”‚
â”‚  â””â”€â†’ + Sample data (data_generator)            â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                â”‚
             â”‚                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Data         â”‚   â”‚ Social Media       â”‚
    â”‚ Generator    â”‚   â”‚ Scraper            â”‚
    â”‚              â”‚   â”‚                    â”‚
    â”‚ â€¢ Zones      â”‚   â”‚ â€¢ Reddit API       â”‚
    â”‚ â€¢ Floods     â”‚   â”‚ â€¢ Twitter/Nitter   â”‚
    â”‚ â€¢ Infra      â”‚   â”‚ â€¢ News RSS         â”‚
    â”‚ â€¢ Sample     â”‚   â”‚ â€¢ Background       â”‚
    â”‚   Posts      â”‚   â”‚   Thread (60s)     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Summary

| Data Type | Source | Status |
|-----------|--------|--------|
| **Statistics** | Hardcoded in `main.py` | Sample |
| **Disaster Zones** | Generated by `data_generator.py` | Simulated |
| **Flood Areas** | Generated by `data_generator.py` | Simulated |
| **Infrastructure** | Generated by `data_generator.py` | Simulated |
| **Social Media** | Real (Reddit/Twitter/News) + Sample | **REAL + Sample** |

---

## ğŸ”§ How to Make It Fully Real

### **Option 1: Connect to Real Disaster Database**
```python
# Replace data_generator with real database
import psycopg2

def get_real_disaster_zones():
    conn = psycopg2.connect("postgresql://disaster_db")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM disaster_zones WHERE active = true")
    zones = cursor.fetchall()
    return zones
```

### **Option 2: Integrate with Government APIs**
```python
# Example: USGS Earthquake API
import requests

def get_real_earthquake_data():
    url = "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson"
    response = requests.get(url)
    earthquakes = response.json()
    return earthquakes
```

### **Option 3: Use Satellite Data**
```python
# Example: NASA FIRMS (Fire data)
def get_real_fire_data():
    url = "https://firms.modaps.eosdis.nasa.gov/api/area/csv/..."
    response = requests.get(url)
    fires = parse_csv(response.text)
    return fires
```

---

## ğŸ“ For Demo/Judges

**When asked "Where does this data come from?"**

**Answer:**
"We have a hybrid data architecture. The social media feed uses **real data** from Reddit, Twitter, and news RSS feeds, fetched in the background every 60 seconds. The disaster zones and statistics are currently **simulated** using realistic algorithms, but the platform is designed to integrate with real-time data sources like government APIs, satellite feeds, or emergency service databases. The architecture is production-ready and can be connected to any data source via our modular backend."

**Key Points:**
- âœ… Real social media data (Reddit, Twitter, News)
- âœ… Background caching (60-second refresh)
- âœ… Simulated disaster data (realistic patterns)
- âœ… Production-ready architecture
- âœ… Easy to connect to real APIs

---

## ğŸš€ Current Implementation Status

**What's Real:**
- âœ… Social media scraping (Reddit, Twitter, News)
- âœ… Background caching system
- âœ… API integration architecture

**What's Simulated:**
- ğŸ“Š Disaster zone coordinates
- ğŸ“Š Damage statistics
- ğŸ“Š Infrastructure data

**Why Simulated?**
- For demo purposes (no access to real disaster databases)
- Realistic patterns and distributions
- Can be replaced with real APIs in production

---

# ğŸ¯ Bottom Line

**The platform is designed to handle real data**, but for the hackathon demo, we use a combination of:
1. **Real social media** (to show the scraping works)
2. **Simulated disaster data** (to show the visualization works)

**In production**, you would connect to:
- Government disaster databases
- Satellite imagery APIs
- Emergency service feeds
- IoT sensor networks
- Drone data streams

**The architecture supports all of this!** ğŸš€
