# 📊 Where Does the Data Come From?

## 🎯 Quick Answer

The data you see in the Control Panel comes from **two sources**:

1. **Sample/Simulated Data** - Generated by `backend/data_generator.py`
2. **Real Data** - Fetched from Reddit, Twitter, and News RSS feeds

---

## 📈 Control Panel Statistics

### **What You See:**
```
Damaged Buildings: 1,247
Flooded Zones: 18
Displaced People: 12,500
Emergency Shelters: 15
```

### **Where It Comes From:**

**File:** `backend/main.py` (lines 230-242)

```python
@app.get("/api/statistics")
async def get_statistics():
    """Get disaster statistics dashboard"""
    stats = {
        "total_affected_area_km2": 245.7,
        "damaged_buildings": 1247,        # ← This number
        "flooded_zones": 18,              # ← This number
        "displaced_population": 12500,    # ← This number
        "rescue_operations_active": 23,
        "emergency_shelters": 15,         # ← This number
        "last_updated": datetime.now().isoformat()
    }
    return stats
```

**Status:** These are **hardcoded sample values** for demo purposes.

---

## 🗺️ Map Data (Zones, Floods, Infrastructure)

### **What You See:**
- Red markers: Disaster zones
- Blue markers: Flood areas
- Orange markers: Infrastructure damage
- Purple markers: Displacement zones

### **Where It Comes From:**

**File:** `backend/data_generator.py`

```python
class DataGenerator:
    def generate_disaster_zones(self):
        """Generate realistic disaster zone data"""
        zones = []
        for i in range(15):
            zone = {
                "id": f"zone_{i+1}",
                "name": f"Zone {chr(65+i)}",
                "severity": random.choice(["critical", "high", "medium", "low"]),
                "damage_score": random.uniform(0.3, 0.95),
                "affected_area_km2": random.uniform(5, 50),
                "coordinates": {
                    "lat": 19.0760 + random.uniform(-0.1, 0.1),
                    "lon": 72.8777 + random.uniform(-0.1, 0.1)
                }
            }
            zones.append(zone)
        return zones
```

**Status:** These are **algorithmically generated** based on realistic patterns.

---

## 📱 Social Media Feed

### **What You See:**
Posts like:
- "Urgent help needed in Bandra..."
- "Flooding reported near..."
- Reddit posts about disasters

### **Where It Comes From:**

**Two Sources:**

#### **1. Real Data (Background Fetched)**
**File:** `backend/social_media_scraper.py`

```python
class SocialMediaScraper:
    def get_all_social_media(self):
        """Fetch REAL data from Reddit, Twitter, News"""
        all_posts = []
        
        # Reddit
        reddit_posts = self.scrape_reddit()
        
        # Twitter (via Nitter)
        twitter_posts = self.scrape_nitter()
        
        # News RSS
        news_posts = self.scrape_news_rss()
        
        return {
            'posts': all_posts,
            'sources': ['Reddit', 'Twitter', 'News RSS']
        }
```

**How it works:**
- Runs in background thread (every 60 seconds)
- Fetches from Reddit API
- Scrapes Twitter via Nitter
- Parses news RSS feeds
- Caches results for fast frontend access

#### **2. Sample Data (Instant)**
**File:** `backend/data_generator.py`

```python
def generate_social_feed(self):
    """Generate sample social media posts"""
    posts = []
    templates = [
        "Urgent help needed in {location}. {people} people trapped.",
        "Flooding reported near {location}. Water level rising.",
        "Building collapsed in {location}. Rescue teams needed."
    ]
    
    for template in templates:
        post = {
            "text": template.format(
                location=random.choice(locations),
                people=random.randint(5, 50)
            ),
            "urgency": random.choice(["critical", "high", "medium"]),
            "timestamp": datetime.now().isoformat()
        }
        posts.append(post)
    
    return posts
```

**Combined in API:**
```python
@app.get("/api/social-feed")
async def get_social_feed():
    # Get cached REAL data
    real_posts = social_media_cache.get("posts", [])
    
    # Get SAMPLE data
    sample_posts = data_generator.generate_social_feed()
    
    # Combine both
    all_posts = real_posts + sample_posts
    
    return {
        "posts": all_posts,
        "real_count": len(real_posts),
        "sample_count": len(sample_posts)
    }
```

---

## 🔄 Data Flow Diagram

```
┌─────────────────────────────────────────────────┐
│                  FRONTEND                       │
│  (React App - http://localhost:3000)           │
└────────────────┬────────────────────────────────┘
                 │
                 │ HTTP Requests
                 ↓
┌─────────────────────────────────────────────────┐
│              BACKEND API                        │
│  (FastAPI - http://localhost:8000)             │
├─────────────────────────────────────────────────┤
│                                                 │
│  GET /api/statistics                           │
│  ├─→ Returns hardcoded stats                   │
│                                                 │
│  GET /api/disaster-zones                       │
│  ├─→ data_generator.generate_disaster_zones()  │
│                                                 │
│  GET /api/social-feed                          │
│  ├─→ Cached real data (background thread)      │
│  └─→ + Sample data (data_generator)            │
│                                                 │
└────────────┬────────────────┬───────────────────┘
             │                │
             │                │
    ┌────────▼─────┐   ┌─────▼──────────────┐
    │ Data         │   │ Social Media       │
    │ Generator    │   │ Scraper            │
    │              │   │                    │
    │ • Zones      │   │ • Reddit API       │
    │ • Floods     │   │ • Twitter/Nitter   │
    │ • Infra      │   │ • News RSS         │
    │ • Sample     │   │ • Background       │
    │   Posts      │   │   Thread (60s)     │
    └──────────────┘   └────────────────────┘
```

---

## 🎯 Summary

| Data Type | Source | Status |
|-----------|--------|--------|
| **Statistics** | Hardcoded in `main.py` | Sample |
| **Disaster Zones** | Generated by `data_generator.py` | Simulated |
| **Flood Areas** | Generated by `data_generator.py` | Simulated |
| **Infrastructure** | Generated by `data_generator.py` | Simulated |
| **Social Media** | Real (Reddit/Twitter/News) + Sample | **REAL + Sample** |

---

## 🔧 How to Make It Fully Real

### **Option 1: Connect to Real Disaster Database**
```python
# Replace data_generator with real database
import psycopg2

def get_real_disaster_zones():
    conn = psycopg2.connect("postgresql://disaster_db")
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM disaster_zones WHERE active = true")
    zones = cursor.fetchall()
    return zones
```

### **Option 2: Integrate with Government APIs**
```python
# Example: USGS Earthquake API
import requests

def get_real_earthquake_data():
    url = "https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson"
    response = requests.get(url)
    earthquakes = response.json()
    return earthquakes
```

### **Option 3: Use Satellite Data**
```python
# Example: NASA FIRMS (Fire data)
def get_real_fire_data():
    url = "https://firms.modaps.eosdis.nasa.gov/api/area/csv/..."
    response = requests.get(url)
    fires = parse_csv(response.text)
    return fires
```

---

## 📝 For Demo/Judges

**When asked "Where does this data come from?"**

**Answer:**
"We have a hybrid data architecture. The social media feed uses **real data** from Reddit, Twitter, and news RSS feeds, fetched in the background every 60 seconds. The disaster zones and statistics are currently **simulated** using realistic algorithms, but the platform is designed to integrate with real-time data sources like government APIs, satellite feeds, or emergency service databases. The architecture is production-ready and can be connected to any data source via our modular backend."

**Key Points:**
- ✅ Real social media data (Reddit, Twitter, News)
- ✅ Background caching (60-second refresh)
- ✅ Simulated disaster data (realistic patterns)
- ✅ Production-ready architecture
- ✅ Easy to connect to real APIs

---

## 🚀 Current Implementation Status

**What's Real:**
- ✅ Social media scraping (Reddit, Twitter, News)
- ✅ Background caching system
- ✅ API integration architecture

**What's Simulated:**
- 📊 Disaster zone coordinates
- 📊 Damage statistics
- 📊 Infrastructure data

**Why Simulated?**
- For demo purposes (no access to real disaster databases)
- Realistic patterns and distributions
- Can be replaced with real APIs in production

---

# 🎯 Bottom Line

**The platform is designed to handle real data**, but for the hackathon demo, we use a combination of:
1. **Real social media** (to show the scraping works)
2. **Simulated disaster data** (to show the visualization works)

**In production**, you would connect to:
- Government disaster databases
- Satellite imagery APIs
- Emergency service feeds
- IoT sensor networks
- Drone data streams

**The architecture supports all of this!** 🚀
